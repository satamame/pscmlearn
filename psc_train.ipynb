{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python3\n",
    "# encoding: utf-8\n",
    "\n",
    "# 特徴量データ + 教師ラベルのリストを入力として、予測モデルを出力するプログラム。\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "\n",
    "import psclib.psc as psc\n",
    "\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import Link, Chain, ChainList, Variable\n",
    "from chainer import iterators, optimizers\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer.datasets import split_dataset_random\n",
    "from chainer.dataset import concat_examples\n",
    "from chainer.cuda import to_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "args = sys.argv\n",
    "if len(args) < 4:\n",
    "    print('Usage: python psc_train.py train_list_file, test_list_file, model_file')\n",
    "    sys.exit()\n",
    "\"\"\"\n",
    "\n",
    "args = [\n",
    "    \"psc_train.py\",\n",
    "    \"ds_train_list.txt\",\n",
    "    \"ds_test_list.txt\",\n",
    "    \"model/mdl_0000.pkl\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(list_file):\n",
    "    \"\"\"\n",
    "    特徴量データと教師ラベルが対になったデータセットを作成\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_file : string\n",
    "        入力データのリストのファイル名\n",
    "        各行が、\"特徴量データのファイル名, 教師ラベルのファイル名\" の形式\n",
    "    \"\"\"\n",
    "\n",
    "    # 「特徴量データ, 教師ラベル」 のファイル名リストを読み込む\n",
    "    ft_files = []\n",
    "    lbl_files = []\n",
    "    try:\n",
    "        for line in open(list_file, 'r'):\n",
    "            ft, lbl = line.split(',')\n",
    "            ft_files.append(ft.strip())\n",
    "            lbl_files.append(lbl.strip())\n",
    "    except IOError as err:\n",
    "        print(err)\n",
    "        sys.exit()\n",
    "    except ValueError as err:\n",
    "        print(err)\n",
    "        sys.exit()\n",
    "\n",
    "    # 特徴量データのリストを作成\n",
    "    in_fts = []\n",
    "    for ft_file in ft_files:\n",
    "        for line in open(ft_file, 'r'):\n",
    "            fts = [float(f) for f in line.split(',')]\n",
    "            in_fts.append(fts)\n",
    "\n",
    "    # 教師ラベル (数値に変換したもの) のリストを作成\n",
    "    in_lbls = []\n",
    "    try:\n",
    "        for lbl_file in lbl_files:\n",
    "            for line in open(lbl_file, 'r'):\n",
    "                in_lbls.append(psc.classes.index(line.strip()))\n",
    "    except ValueError as err:\n",
    "        print(err)\n",
    "        sys.exit()\n",
    "\n",
    "    dataset = list(zip(in_fts, in_lbls))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = make_dataset(args[1])\n",
    "ds_test = make_dataset(args[2])\n",
    "\n",
    "# __TODO__ BOM があるとエラーになるので何とかする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ds_train))\n",
    "print(len(ds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クラスは、あとで psc パッケージに移す\n",
    "class PscChain(Chain):\n",
    "    def __init__(self, hid_dim, out_dim):\n",
    "        \"\"\"\n",
    "        初期化メソッド\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hid_dim : integer\n",
    "            隠れ層のノード数\n",
    "        out_dim : integer\n",
    "            出力層のノード数\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            l1=L.Linear(None, hid_dim),\n",
    "            l2=L.Linear(hid_dim, hid_dim),\n",
    "            l3=L.Linear(hid_dim, out_dim)\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        順伝播して、出力層 (Variable) を返す\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Variable\n",
    "            (バッチサイズ x 特徴ベクトルの次元数) の、入力データ\n",
    "        \"\"\"\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)\n",
    "\n",
    "        # ここで softmax して、確率にして返すこと。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量データと教師ラベルを対にして、学習用と検証用に分ける\n",
    "train_count = int(len(ds_train) * 0.8) # 8割のデータを学習用に\n",
    "ds_train, ds_valid = split_dataset_random(ds_train, train_count, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ds_train))\n",
    "print(len(ds_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル定義\n",
    "hid_dim = 20                # 隠れ層のノード数 : いい塩梅に決める\n",
    "out_dim = len(psc.classes)  # 出力層のノード数 : 定義されているラベルの数\n",
    "\n",
    "model = PscChain(hid_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチサイズ\n",
    "batch_size = 100\n",
    "\n",
    "# 学習用イテレータ\n",
    "train_iter = iterators.SerialIterator(ds_train, batch_size)\n",
    "# 検証用イテレータ\n",
    "valid_iter = iterators.SerialIterator(ds_valid, batch_size, repeat=False, shuffle=False)\n",
    "\n",
    "# Optimizer の setup\n",
    "optimizer = optimizers.SGD(lr=0.01).setup(model)\n",
    "\n",
    "# GPU を使う (使うなら 0, 使わないなら -1)\n",
    "gpu_id = 0\n",
    "\n",
    "if gpu_id >= 0:\n",
    "    model.to_gpu(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エポック数\n",
    "max_epoch = 100\n",
    "\n",
    "while train_iter.epoch < max_epoch:\n",
    "    # イテレーション\n",
    "    train_batch = train_iter.next()\n",
    "    \n",
    "    # 1 イテレーション分の、入力データの array と、教師ラベルの array\n",
    "    x, t = concat_examples(train_batch, gpu_id)\n",
    "    x = x.astype(np.float32)\n",
    "    \n",
    "    # 順伝播の結果を得る\n",
    "    y = model(x)\n",
    "    \n",
    "    # ロスの計算\n",
    "    loss = F.softmax_cross_entropy(y, t)\n",
    "    \n",
    "    # 勾配の計算\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    \n",
    "    # パラメータの更新\n",
    "    optimizer.update()\n",
    "    \n",
    "    if train_iter.is_new_epoch:  # 1 epochが終わったら\n",
    "\n",
    "        # ロスの表示\n",
    "        # print('epoch:{:02d} train_loss:{:.04f} '.format(\n",
    "        #    train_iter.epoch, float(to_cpu(loss.data))), end='')\n",
    "        \n",
    "        valid_losses = []\n",
    "        valid_accuracies = []\n",
    "        while True:\n",
    "            valid_batch = valid_iter.next()\n",
    "            x_valid, t_valid = concat_examples(valid_batch, gpu_id)\n",
    "            x_valid = x_valid.astype(np.float32)\n",
    "\n",
    "            # Validationデータをforward\n",
    "            with chainer.using_config('train', False), \\\n",
    "                    chainer.using_config('enable_backprop', False):\n",
    "                y_valid = model(x_valid)\n",
    "                \n",
    "            # print(y_valid)\n",
    "\n",
    "            # ロスを計算\n",
    "            loss_valid = F.softmax_cross_entropy(y_valid, t_valid)\n",
    "            valid_losses.append(to_cpu(loss_valid.array))\n",
    "\n",
    "            # 精度を計算\n",
    "            accuracy = F.accuracy(y_valid, t_valid)\n",
    "            accuracy.to_cpu()\n",
    "            valid_accuracies.append(accuracy.array)\n",
    "\n",
    "            if valid_iter.is_new_epoch:\n",
    "                valid_iter.reset()\n",
    "                break\n",
    "\n",
    "        print('val_loss:{:.04f} val_accuracy:{:.04f}'.format(\n",
    "            np.mean(valid_losses), np.mean(valid_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価用イテレータ\n",
    "test_iter = iterators.SerialIterator(ds_test, batch_size, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータでの評価\n",
    "test_accuracies = []\n",
    "while True:\n",
    "    test_batch = test_iter.next()\n",
    "    x_test, t_test = concat_examples(test_batch, gpu_id)\n",
    "    x_test = x_test.astype(np.float32)\n",
    "    \n",
    "    # print(x_test)\n",
    "\n",
    "    # テストデータをforward\n",
    "    with chainer.using_config('train', False), \\\n",
    "            chainer.using_config('enable_backprop', False):\n",
    "        y_test = model(x_test)\n",
    "    \n",
    "    # print(y_test)\n",
    "    \n",
    "    \"\"\"\n",
    "    for i, a in enumerate(y_test):\n",
    "        print(a, t_test[i])\n",
    "    \"\"\"\n",
    "    # 精度を計算\n",
    "    accuracy = F.accuracy(y_test, t_test)\n",
    "    accuracy.to_cpu()\n",
    "    test_accuracies.append(accuracy.array)\n",
    "\n",
    "    if test_iter.is_new_epoch:\n",
    "        test_iter.reset()\n",
    "        break\n",
    "\n",
    "print('test_accuracy:{:.04f}'.format(np.mean(test_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
