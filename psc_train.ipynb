{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python3\n",
    "# encoding: utf-8\n",
    "\n",
    "# 学習用データ, 評価用データを入力として、学習した予測モデルを出力するプログラム。\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "import psclib.psc as psc\n",
    "\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import Link, ChainList, Variable\n",
    "from chainer import iterators, optimizers\n",
    "import chainer.functions as F\n",
    "from chainer.datasets import split_dataset_random\n",
    "from chainer.dataset import concat_examples\n",
    "from chainer.cuda import to_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "args = sys.argv\n",
    "if len(args) < 4:\n",
    "    print('Usage: python psc_train.py train_list_file, eval_list_file, model_save_file')\n",
    "    sys.exit()\n",
    "\"\"\"\n",
    "args = [\n",
    "    \"psc_train.py\",\n",
    "    \"ds_train_list.txt\",\n",
    "    \"ds_eval_list.txt\",\n",
    "    \"model/mdl_0000.pkl\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメタを再定義\n",
    "train_list_file = args[1]\n",
    "eval_list_file = args[2]\n",
    "model_save_file = args[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(list_file):\n",
    "    \"\"\"\n",
    "    特徴量データと教師ラベルが対になったデータセットを作成\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_file : string\n",
    "        入力データのリストのファイル名\n",
    "        ファイルの各行が、\"特徴量データのファイル名, 教師ラベルのファイル名\" の形式\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dataset : list\n",
    "        (特徴量データ, 教師ラベル) というタプルを要素に持つリスト\n",
    "    \"\"\"\n",
    "\n",
    "    # 「特徴量データ, 教師ラベル」 のファイル名リストを読み込む\n",
    "    ft_files = []\n",
    "    lbl_files = []\n",
    "    with open(list_file, 'r', encoding='utf_8_sig') as f:\n",
    "        for line in f:\n",
    "            ft, lbl = line.split(',')\n",
    "            ft_files.append(ft.strip())\n",
    "            lbl_files.append(lbl.strip())\n",
    "\n",
    "    # 特徴量データのリストを作成\n",
    "    in_fts = []\n",
    "    for ft_file in ft_files:\n",
    "        with open(ft_file, 'r') as f:\n",
    "            reader = csv.reader(f, quoting=csv.QUOTE_NONNUMERIC)\n",
    "            in_fts.extend([[float(v) for v in row] for row in reader])\n",
    "        \n",
    "    # 教師ラベル (数値に変換したもの) のリストを作成\n",
    "    in_lbls = []\n",
    "    for lbl_file in lbl_files:\n",
    "        with open(lbl_file, 'r', encoding='utf_8_sig') as f:\n",
    "            in_lbls.extend([psc.classes.index(line.strip()) for line in f])\n",
    "\n",
    "    dataset = list(zip(in_fts, in_lbls))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データセット\n",
    "ds_train = make_dataset(train_list_file)\n",
    "# 評価用データセット\n",
    "ds_eval = make_dataset(eval_list_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量データと教師ラベルを対にして、学習用と検証用に分ける\n",
    "train_count = int(len(ds_train) * 0.8) # 8割のデータを学習用に\n",
    "ds_train, ds_valid = split_dataset_random(ds_train, train_count, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル定義\n",
    "hid_dim = 20                # 隠れ層のノード数 : いい塩梅に決める\n",
    "out_dim = len(psc.classes)  # 出力層のノード数 : 定義されているラベルの数\n",
    "\n",
    "model = psc.PscChain(hid_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチサイズ\n",
    "batch_size = 100\n",
    "\n",
    "# 学習用イテレータ\n",
    "train_iter = iterators.SerialIterator(ds_train, batch_size)\n",
    "# 検証用イテレータ\n",
    "valid_iter = iterators.SerialIterator(ds_valid, batch_size, repeat=False, shuffle=False)\n",
    "\n",
    "# Optimizer の setup\n",
    "optimizer = optimizers.SGD(lr=0.01).setup(model)\n",
    "\n",
    "# GPU を使う (使うなら 0, 使わないなら -1)\n",
    "gpu_id = 0\n",
    "\n",
    "if gpu_id >= 0:\n",
    "    model.to_gpu(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エポック数\n",
    "max_epoch = 40\n",
    "\n",
    "while train_iter.epoch < max_epoch:\n",
    "    # イテレーション\n",
    "    train_batch = train_iter.next()\n",
    "    \n",
    "    # 1 イテレーション分の、入力データの array と、教師ラベルの array\n",
    "    x, t = concat_examples(train_batch, gpu_id)\n",
    "    x = x.astype(np.float32)\n",
    "    \n",
    "    # 順伝播の結果を得る\n",
    "    y = model(x)\n",
    "    \n",
    "    # ロスの計算\n",
    "    loss = F.softmax_cross_entropy(y, t)\n",
    "    \n",
    "    # 勾配の計算\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    \n",
    "    # パラメータの更新\n",
    "    optimizer.update()\n",
    "    \n",
    "    if train_iter.is_new_epoch:  # 1 epochが終わったら\n",
    "\n",
    "        # ロスの表示\n",
    "        # print('epoch:{:02d} train_loss:{:.04f} '.format(\n",
    "        #    train_iter.epoch, float(to_cpu(loss.data))), end='')\n",
    "        \n",
    "        valid_losses = []\n",
    "        valid_accuracies = []\n",
    "        while True:\n",
    "            valid_batch = valid_iter.next()\n",
    "            x_valid, t_valid = concat_examples(valid_batch, gpu_id)\n",
    "            x_valid = x_valid.astype(np.float32)\n",
    "\n",
    "            # Validation データを forward\n",
    "            with chainer.using_config('train', False), \\\n",
    "                    chainer.using_config('enable_backprop', False):\n",
    "                y_valid = model(x_valid)\n",
    "                \n",
    "            # ロスを計算\n",
    "            loss_valid = F.softmax_cross_entropy(y_valid, t_valid)\n",
    "            valid_losses.append(to_cpu(loss_valid.array))\n",
    "\n",
    "            # 精度を計算\n",
    "            accuracy = F.accuracy(y_valid, t_valid)\n",
    "            accuracy.to_cpu()\n",
    "            valid_accuracies.append(accuracy.array)\n",
    "\n",
    "            if valid_iter.is_new_epoch:\n",
    "                valid_iter.reset()\n",
    "                break\n",
    "\n",
    "        # ロスと精度の表示\n",
    "        print('{:0=2} val_loss:{:.04f} val_accuracy:{:.04f}'.format(\n",
    "            train_iter.epoch, np.mean(valid_losses), np.mean(valid_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価用イテレータ\n",
    "eval_iter = iterators.SerialIterator(ds_eval, batch_size, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価用データでの評価\n",
    "eval_accuracies = []\n",
    "while True:\n",
    "    eval_batch = eval_iter.next()\n",
    "    x_eval, t_eval = concat_examples(eval_batch, gpu_id)\n",
    "    x_eval = x_eval.astype(np.float32)\n",
    "    \n",
    "    # 評価用データをforward\n",
    "    with chainer.using_config('train', False), \\\n",
    "            chainer.using_config('enable_backprop', False):\n",
    "        y_eval = model(x_eval)\n",
    "    \n",
    "    # 精度を計算\n",
    "    accuracy = F.accuracy(y_eval, t_eval)\n",
    "    accuracy.to_cpu()\n",
    "    eval_accuracies.append(accuracy.array)\n",
    "\n",
    "    if eval_iter.is_new_epoch:\n",
    "        eval_iter.reset()\n",
    "        break\n",
    "\n",
    "print('eval_accuracy:{:.04f}'.format(np.mean(eval_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを保存する (保存するときは CPU 版とする)\n",
    "with open(model_save_file, 'wb') as f:\n",
    "    pickle.dump(model.to_cpu(), f)\n",
    "print(\"Model is saved as {}.\".format(model_save_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
