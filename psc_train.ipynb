{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python3\n",
    "# encoding: utf-8\n",
    "\n",
    "# 学習用データ, 評価用データを入力として、学習した予測モデルを出力するプログラム。\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "# import random # 不要か？\n",
    "import pickle\n",
    "\n",
    "import psclib.psc as psc\n",
    "\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import Link, ChainList, Variable\n",
    "# from chainer import Chain\n",
    "from chainer import iterators, optimizers\n",
    "# import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer.datasets import split_dataset_random\n",
    "from chainer.dataset import concat_examples\n",
    "from chainer.cuda import to_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "args = sys.argv\n",
    "if len(args) < 4:\n",
    "    print('Usage: python psc_train.py train_list_file, eval_list_file, model_file')\n",
    "    sys.exit()\n",
    "\"\"\"\n",
    "args = [\n",
    "    \"psc_train.py\",\n",
    "    \"ds_train_list.txt\",\n",
    "    \"ds_eval_list.txt\",\n",
    "    \"model/mdl_0000.pkl\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_file = args[1]\n",
    "eval_list_file = args[2]\n",
    "model_save_file = args[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(list_file):\n",
    "    \"\"\"\n",
    "    特徴量データと教師ラベルが対になったデータセットを作成\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_file : string\n",
    "        入力データのリストのファイル名\n",
    "        各行が、\"特徴量データのファイル名, 教師ラベルのファイル名\" の形式\n",
    "    \"\"\"\n",
    "\n",
    "    # 「特徴量データ, 教師ラベル」 のファイル名リストを読み込む\n",
    "    ft_files = []\n",
    "    lbl_files = []\n",
    "    try:\n",
    "        for line in open(list_file, 'r'):\n",
    "            ft, lbl = line.split(',')\n",
    "            ft_files.append(ft.strip())\n",
    "            lbl_files.append(lbl.strip())\n",
    "    except IOError as err:\n",
    "        print(err)\n",
    "        sys.exit()\n",
    "    except ValueError as err:\n",
    "        print(err)\n",
    "        sys.exit()\n",
    "\n",
    "    # 特徴量データのリストを作成\n",
    "    in_fts = []\n",
    "    for ft_file in ft_files:\n",
    "        for line in open(ft_file, 'r'):\n",
    "            fts = [float(f) for f in line.split(',')]\n",
    "            in_fts.append(fts)\n",
    "\n",
    "    # 教師ラベル (数値に変換したもの) のリストを作成\n",
    "    in_lbls = []\n",
    "    try:\n",
    "        for lbl_file in lbl_files:\n",
    "            for line in open(lbl_file, 'r'):\n",
    "                in_lbls.append(psc.classes.index(line.strip()))\n",
    "    except ValueError as err:\n",
    "        print(err)\n",
    "        sys.exit()\n",
    "\n",
    "    dataset = list(zip(in_fts, in_lbls))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = make_dataset(train_list_file)\n",
    "ds_eval = make_dataset(eval_list_file)\n",
    "\n",
    "# __TODO__ BOM があるとエラーになるので何とかする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ds_train))\n",
    "print(len(ds_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量データと教師ラベルを対にして、学習用と検証用に分ける\n",
    "train_count = int(len(ds_train) * 0.8) # 8割のデータを学習用に\n",
    "ds_train, ds_valid = split_dataset_random(ds_train, train_count, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ds_train))\n",
    "print(len(ds_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル定義\n",
    "hid_dim = 20                # 隠れ層のノード数 : いい塩梅に決める\n",
    "out_dim = len(psc.classes)  # 出力層のノード数 : 定義されているラベルの数\n",
    "\n",
    "model = psc.PscChain(hid_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチサイズ\n",
    "batch_size = 100\n",
    "\n",
    "# 学習用イテレータ\n",
    "train_iter = iterators.SerialIterator(ds_train, batch_size)\n",
    "# 検証用イテレータ\n",
    "valid_iter = iterators.SerialIterator(ds_valid, batch_size, repeat=False, shuffle=False)\n",
    "\n",
    "# Optimizer の setup\n",
    "optimizer = optimizers.SGD(lr=0.01).setup(model)\n",
    "\n",
    "# GPU を使う (使うなら 0, 使わないなら -1)\n",
    "gpu_id = 0\n",
    "\n",
    "if gpu_id >= 0:\n",
    "    model.to_gpu(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エポック数\n",
    "max_epoch = 40\n",
    "\n",
    "while train_iter.epoch < max_epoch:\n",
    "    # イテレーション\n",
    "    train_batch = train_iter.next()\n",
    "    \n",
    "    # 1 イテレーション分の、入力データの array と、教師ラベルの array\n",
    "    x, t = concat_examples(train_batch, gpu_id)\n",
    "    x = x.astype(np.float32)\n",
    "    \n",
    "    print(type(x))\n",
    "    print(x.shape)\n",
    "    \n",
    "    # 順伝播の結果を得る\n",
    "    y = model(x)\n",
    "    \n",
    "    # ロスの計算\n",
    "    loss = F.softmax_cross_entropy(y, t)\n",
    "    \n",
    "    # 勾配の計算\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    \n",
    "    # パラメータの更新\n",
    "    optimizer.update()\n",
    "    \n",
    "    if train_iter.is_new_epoch:  # 1 epochが終わったら\n",
    "\n",
    "        # ロスの表示\n",
    "        # print('epoch:{:02d} train_loss:{:.04f} '.format(\n",
    "        #    train_iter.epoch, float(to_cpu(loss.data))), end='')\n",
    "        \n",
    "        valid_losses = []\n",
    "        valid_accuracies = []\n",
    "        while True:\n",
    "            valid_batch = valid_iter.next()\n",
    "            x_valid, t_valid = concat_examples(valid_batch, gpu_id)\n",
    "            x_valid = x_valid.astype(np.float32)\n",
    "\n",
    "            # Validationデータをforward\n",
    "            with chainer.using_config('train', False), \\\n",
    "                    chainer.using_config('enable_backprop', False):\n",
    "                y_valid = model(x_valid)\n",
    "                \n",
    "            # print(y_valid)\n",
    "\n",
    "            # ロスを計算\n",
    "            loss_valid = F.softmax_cross_entropy(y_valid, t_valid)\n",
    "            valid_losses.append(to_cpu(loss_valid.array))\n",
    "\n",
    "            # 精度を計算\n",
    "            accuracy = F.accuracy(y_valid, t_valid)\n",
    "            accuracy.to_cpu()\n",
    "            valid_accuracies.append(accuracy.array)\n",
    "\n",
    "            if valid_iter.is_new_epoch:\n",
    "                valid_iter.reset()\n",
    "                break\n",
    "\n",
    "        print('{:0=2} val_loss:{:.04f} val_accuracy:{:.04f}'.format(\n",
    "            train_iter.epoch, np.mean(valid_losses), np.mean(valid_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価用イテレータ\n",
    "eval_iter = iterators.SerialIterator(ds_eval, batch_size, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価用データでの評価\n",
    "eval_accuracies = []\n",
    "while True:\n",
    "    eval_batch = eval_iter.next()\n",
    "    x_eval, t_eval = concat_examples(eval_batch, gpu_id)\n",
    "    x_eval = x_eval.astype(np.float32)\n",
    "    \n",
    "    # print(x_eval)\n",
    "\n",
    "    # 評価用データをforward\n",
    "    with chainer.using_config('train', False), \\\n",
    "            chainer.using_config('enable_backprop', False):\n",
    "        y_eval = model(x_eval)\n",
    "    \n",
    "    # print(y_eval)\n",
    "    \n",
    "    \"\"\"\n",
    "    for i, a in enumerate(y_eval):\n",
    "        print(a, t_eval[i])\n",
    "    \"\"\"\n",
    "    # 精度を計算\n",
    "    accuracy = F.accuracy(y_eval, t_eval)\n",
    "    accuracy.to_cpu()\n",
    "    eval_accuracies.append(accuracy.array)\n",
    "\n",
    "    if eval_iter.is_new_epoch:\n",
    "        eval_iter.reset()\n",
    "        break\n",
    "\n",
    "print('eval_accuracy:{:.04f}'.format(np.mean(eval_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを保存する (保存するときは CPU 版とする)\n",
    "with open(model_save_file, 'wb') as f:\n",
    "    pickle.dump(model.to_cpu(), f)\n",
    "print(\"Model is saved as {}.\".format(model_save_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
